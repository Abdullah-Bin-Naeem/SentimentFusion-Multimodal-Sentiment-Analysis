{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SentimentFusion: Multimodal Sentiment Analysis**"
      ],
      "metadata": {
        "id": "BYw1cUu-fkUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries**"
      ],
      "metadata": {
        "id": "ccJ0QWvSfUec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "#import transformers\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torchvision.transforms import transforms\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9huH4U2fRYi",
        "outputId": "1c70c06b-803f-451d-b3fe-3273588db035"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "!cd /content/drive/MyDrive/AIProject/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moC8_vPVcXrY",
        "outputId": "24c5d72d-8436-4317-8944-8312f38ab6ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/AIProject/sentiment.csv\")"
      ],
      "metadata": {
        "id": "BUlz0N8IdQZ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Op6VO2QekbF-",
        "outputId": "e12df91d-5f1b-43d0-a74f-964d4a79222f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  imgid  split                       filename  successful  \\\n",
              "0           0  31369  train  COCO_val2014_000000389081.jpg           1   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  ['a', 'plate', 'of', 'delicious', 'food', 'inc...   \n",
              "\n",
              "                         word_sentiment  sentiment  \\\n",
              "0  [0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0]          1   \n",
              "\n",
              "                                                 raw  \n",
              "0  a plate of delicious food including French fries.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47b81f72-e5ed-4588-a976-5c71d727ab73\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>imgid</th>\n",
              "      <th>split</th>\n",
              "      <th>filename</th>\n",
              "      <th>successful</th>\n",
              "      <th>tokens</th>\n",
              "      <th>word_sentiment</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>31369</td>\n",
              "      <td>train</td>\n",
              "      <td>COCO_val2014_000000389081.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>['a', 'plate', 'of', 'delicious', 'food', 'inc...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0]</td>\n",
              "      <td>1</td>\n",
              "      <td>a plate of delicious food including French fries.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47b81f72-e5ed-4588-a976-5c71d727ab73')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47b81f72-e5ed-4588-a976-5c71d727ab73 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47b81f72-e5ed-4588-a976-5c71d727ab73');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 39199,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11315,\n        \"min\": 0,\n        \"max\": 39198,\n        \"num_unique_values\": 39199,\n        \"samples\": [\n          9992,\n          28956,\n          16224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"imgid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11856,\n        \"min\": 39,\n        \"max\": 40376,\n        \"num_unique_values\": 2225,\n        \"samples\": [\n          8653,\n          39232,\n          20467\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"train\",\n          \"val\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2225,\n        \"samples\": [\n          \"COCO_val2014_000000482080.jpg\",\n          \"COCO_val2014_000000433154.jpg\",\n          \"COCO_val2014_000000153956.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"successful\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8636,\n        \"samples\": [\n          \"['a', 'large', 'beer', 'truck', 'is', 'parked', 'in', 'front', 'of', 'an', 'ugly', 'building']\",\n          \"['a', 'scared', 'cat', 'sits', 'on', 'a', 'bench', 'looking', 'around', 'for', 'someone']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1041,\n        \"samples\": [\n          \"[0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\",\n          \"[0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"raw\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8785,\n        \"samples\": [\n          \"a tall damaged building with a clock embedded at the top\",\n          \"this picture of a farm and its fields was taken on a gorgeous day\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=data[data.successful==1]\n",
        "train_data= data[data.split=='train']\n",
        "test_data= data[data.split=='test']\n",
        "\n",
        "image_folder = '/content/drive/MyDrive/AIProject/sentiment_images/'\n",
        "train_image_paths = [image_folder + img_name for img_name in train_data.filename]\n",
        "test_image_paths = [image_folder + img_name for img_name in test_data.filename]\n",
        "\n",
        "train_captions= list(train_data.raw)\n",
        "test_captions= list(test_data.raw)"
      ],
      "metadata": {
        "id": "Qd6LwUVNj0r_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction using Resent-50 and BERT:**"
      ],
      "metadata": {
        "id": "q0b8R66LVpWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ResNet model\n",
        "resnet = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "# Remove the last layer\n",
        "# modules = list(resnet.children())[:-1]\n",
        "# resnet = torch.nn.Sequential(*modules)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "resnet.eval()\n",
        "\n",
        "# Define transformations to be applied to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define function to extract features from an image\n",
        "def extract_features(image):\n",
        "    # Convert grayscale images to color images\n",
        "    if image.mode == 'L':\n",
        "        image = Image.merge('RGB', [image] * 3)\n",
        "\n",
        "    # Apply transformations\n",
        "    image = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Pass the image through the model\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image)\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgbVpVZHexCi",
        "outputId": "e2898a79-d5db-4fb0-8b08-054821a0f5b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,file in enumerate(test_data.filename):\n",
        "#     image = Image.open(\"sentiment_images/\"+file)\n",
        "\n",
        "#     if i==0:\n",
        "#         with torch.no_grad():\n",
        "#             image_features=extract_features(image).reshape(1000)\n",
        "#     else :\n",
        "#         with torch.no_grad():\n",
        "#             image_features=torch.concat([image_features,extract_features(image).reshape(1000)])\n",
        "#     if i%100==0:\n",
        "#         print(i)\n",
        "# torch.save(image_features,\"testimgfeatures.pt\")\n",
        "test_image_features=torch.load(\"/content/drive/MyDrive/AIProject/testimgfeatures.pt\")"
      ],
      "metadata": {
        "id": "vbFFrOD7mPuL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,file in enumerate(train_data.filename):\n",
        "#     image = Image.open(\"sentiment_images/\"+file)\n",
        "\n",
        "#     if i==0:\n",
        "#         with torch.no_grad():\n",
        "#             image_features=extract_features(image).reshape(1000)\n",
        "#     else :\n",
        "#         with torch.no_grad():\n",
        "#             image_features=torch.concat([image_features,extract_features(image).reshape(1000)])\n",
        "#     if i%100==0:\n",
        "#         print(i)\n",
        "# torch.save(image_features,\"trainimgfeatures.pt\")\n",
        "train_image_features=torch.load(\"/content/drive/MyDrive/AIProject/trainimgfeatures.pt\")"
      ],
      "metadata": {
        "id": "SGQdA2aEmVbr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertTokenizer, BertModel\n",
        "\n",
        "\n",
        "# # Load pre-trained BERT tokenizer and model\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Load text dataset\n",
        "\n",
        "# # Initialize tensor of zeros with appropriate shape\n",
        "# features = torch.zeros((len(test_data), 768))\n",
        "\n",
        "# # Loop over texts and extract features\n",
        "# for i, text in enumerate(test_data.raw):\n",
        "#     # Tokenize text\n",
        "#     tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "#     # Convert tokens to tensor\n",
        "#     input_ids = torch.tensor([tokens])\n",
        "\n",
        "#     # Pass input through BERT model\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids)\n",
        "\n",
        "#     # Extract final hidden state of the first token (CLS token)\n",
        "#     features[i] = outputs.last_hidden_state[0][0]\n",
        "#     if i % 100==0:\n",
        "#       print(i)\n",
        "\n",
        "# # Print shape of features tensor\n",
        "# print(features.shape) # Output: torch.Size([15912, 768])\n",
        "# torch.save(features,\"testtextfeatures.pt\")\n",
        "test_text_features=torch.load(\"/content/drive/MyDrive/AIProject/testtextfeatures.pt\")\n"
      ],
      "metadata": {
        "id": "itkzM4xCmVis"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features = torch.zeros((len(train_data), 768))\n",
        "# for i, text in enumerate(train_data.raw):\n",
        "#     # Tokenize text\n",
        "#     tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "#     # Convert tokens to tensor\n",
        "#     input_ids = torch.tensor([tokens])\n",
        "\n",
        "#     # Pass input through BERT model\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids)\n",
        "\n",
        "#     # Extract final hidden state of the first token (CLS token)\n",
        "#     features[i] = outputs.last_hidden_state[0][0]\n",
        "#     if i % 100==0:\n",
        "#       print(i)\n",
        "\n",
        "# # Print shape of features tensor\n",
        "# print(features.shape) # Output: torch.Size([15912, 768])\n",
        "# torch.save(features,\"traintextfeatures.pt\")\n",
        "train_text_features=torch.load(\"/content/drive/MyDrive/AIProject/traintextfeatures.pt\")"
      ],
      "metadata": {
        "id": "sSYx_38_DCkd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining our MultiModel 1 which uses sentence level features of text and Image features**"
      ],
      "metadata": {
        "id": "EFsa3Jw4WVfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multimodal(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.textMod = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.imageMod = nn.Sequential(\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.lastMod = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "    def forward(self, text, images):\n",
        "        m1 = self.textMod(text)\n",
        "        m2 = self.imageMod(images)\n",
        "        out = torch.cat([m1, m2], dim=1)\n",
        "        out = self.lastMod(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1op_D3wKmVod"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainlabels=torch.tensor(train_data.sentiment,dtype=torch.float32)\n",
        "trainlabels= trainlabels.reshape(len(train_data),1)\n",
        "\n",
        "testlabels=torch.tensor(list(test_data.sentiment),dtype=torch.float32)\n",
        "testlabels= testlabels.reshape(len(test_data),1)"
      ],
      "metadata": {
        "id": "YtT6EIdd9X-b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMuP7V0v_V-U",
        "outputId": "4fca9bc6-bd00-44b1-e505-9a52e11033a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15912000])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_features= train_image_features.reshape(19307,1000)\n",
        "test_image_features= test_image_features.reshape(15912,1000)"
      ],
      "metadata": {
        "id": "a_DxMSaQkv88"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=data_utils.DataLoader(data_utils.TensorDataset(train_image_features,train_text_features,trainlabels),batch_size=32)\n",
        "test_loader=data_utils.DataLoader(data_utils.TensorDataset(test_image_features,test_text_features,testlabels),batch_size=32)"
      ],
      "metadata": {
        "id": "-D57rbqy_f9S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_features.shape, train_text_features.shape,trainlabels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5r32wYj_gPj",
        "outputId": "e0bd61bb-b05a-45a6-c89f-1e246dbe7446"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([19307, 1000]), torch.Size([19307, 768]), torch.Size([19307, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYTC3KIyCWl-",
        "outputId": "3bf8754a-f040-436d-c8a9-e549831b0467"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19307"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod= Multimodal()\n",
        "device = torch.device('cpu')\n",
        "mod.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA1lx8tasxrK",
        "outputId": "c1c8d410-f030-4e9e-be14-83aacef7a441"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Multimodal(\n",
              "  (textMod): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (7): ReLU()\n",
              "  )\n",
              "  (imageMod): Sequential(\n",
              "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (7): ReLU()\n",
              "  )\n",
              "  (lastMod): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              "  (ReLU): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.BCELoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr=learning_rate)\n",
        "epochs = 90"
      ],
      "metadata": {
        "id": "vlauE9JktAq1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "WgTqaQ_QWdT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "mod.train()\n",
        "mod.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = []\n",
        "    for x1, x2, l in train_loader:\n",
        "        x1, x2, l = x1.to(device), x2.to(device), l.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = mod(x2, x1)\n",
        "        loss = loss_func(y_pred, l)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        A = mod(train_text_features, train_image_features) >= 0.5\n",
        "        b = trainlabels >= 0.5\n",
        "        train_ac = torch.sum(b == A).item() / len(train_image_features)\n",
        "\n",
        "        A = mod(test_text_features, test_image_features) >= 0.5\n",
        "        b = testlabels >= 0.5\n",
        "        test_ac = torch.sum(b == A).item() / len(test_image_features)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch', epoch)\n",
        "        print(f'Train Loss: {np.mean(epoch_loss)}  Test Accuracy: {float(test_ac)}  Train Accuracy: {float(train_ac)}')\n",
        "        # print('classification report:\\n', classification_report(np.array(b.cpu()), np.array(A.cpu())))\n",
        "\n",
        "print('Final loss value: ' + str(loss.item()))"
      ],
      "metadata": {
        "id": "VpIUt15miC7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800db78c-b05b-4c6a-a8ba-9b97c04e660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train Loss: 0.6373075234475515  Test Accuracy: 0.5480769230769231  Train Accuracy: 0.5395970373439685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2 for word level textual and Image features multimodal**"
      ],
      "metadata": {
        "id": "Jd1jkg_7UWWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocssing.** Here we are using CountVectorizer with ngrams as our word level textual feature extractor and using the same Image features from Resnet-50"
      ],
      "metadata": {
        "id": "YzpECJzHUnj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/AIProject/sentiment.csv')\n",
        "\n",
        "# Set up the ngram vectorizer\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3),max_features=768)\n",
        "\n",
        "# Fit the vectorizer on the dataset to build the vocabulary\n",
        "ngram_vectorizer.fit(df.raw)\n",
        "\n",
        "train_captions= list(train_data.raw)\n",
        "test_captions= list(test_data.raw)\n",
        "\n",
        "# Transform the dataset to create the feature vectors\n",
        "train_feature_vectors = torch.tensor(ngram_vectorizer.transform(train_captions).toarray())\n",
        "test_feature_vectors = torch.tensor(ngram_vectorizer.transform(test_captions).toarray())\n",
        "\n",
        "# Print the shape of the feature vectors matrix\n",
        "print(train_feature_vectors.shape)\n",
        "print(test_feature_vectors.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "oDXNWxYq_7Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_feature_vectors = train_feature_vectors.to(torch.float32)\n",
        "test_feature_vectors = test_feature_vectors.to(torch.float32)"
      ],
      "metadata": {
        "id": "wu0XutcyRw5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=data_utils.DataLoader(data_utils.TensorDataset(train_image_features,train_feature_vectors,trainlabels),batch_size=32)\n",
        "test_loader=data_utils.DataLoader(data_utils.TensorDataset(test_image_features,test_feature_vectors,testlabels),batch_size=32)"
      ],
      "metadata": {
        "id": "4CivA54bEWuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining our multimodal model**"
      ],
      "metadata": {
        "id": "A9iSOcWiU5CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multimodal2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.textMod = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.imageMod = nn.Sequential(\n",
        "            nn.Linear(1000, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.lastMod = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "    def forward(self, text, images):\n",
        "        m1 = self.textMod(text)\n",
        "        m2 = self.imageMod(images)\n",
        "        out = torch.cat([m1, m2], dim=1)\n",
        "        out = self.lastMod(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "7OiEUYdpLOJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod= Multimodal2()\n",
        "device = torch.device('cpu')\n",
        "mod.to(device)\n",
        "loss_func = nn.BCELoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr=learning_rate)\n",
        "epochs = 1\n"
      ],
      "metadata": {
        "id": "je3Dqx3OLpGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "Rsviy3GSVBiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod.train()\n",
        "for epoch in range(90):\n",
        "    for x1,x2,l in train_loader:\n",
        "        x1,x2,l=x1.to(device),x2.to(device),l.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = mod(x2,x1)\n",
        "        loss = loss_func(y_pred, l)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    A=mod(train_text_features,train_image_features)>=0.5\n",
        "    b=trainlabels>=0.5\n",
        "    train_ac=sum(b==A)/len(train_image_features)\n",
        "    A=mod(test_text_features,test_image_features)>=0.5\n",
        "    b=testlabels>=0.5\n",
        "    test_ac=sum(b==A)/len(test_image_features)\n",
        "    if epoch % 5==0:\n",
        "      print('Epoch',(epoch))\n",
        "      print(f'Train Loss: {np.mean(epoch_loss)}  Test Accuracy: {float(test_ac)}  Train Accuracy: {float(train_ac)}')\n",
        "        #print('clasification report:\\n', classification_report(np.array(b),np.array(A)))\n",
        "print('Final loss value: '+str(loss.item()))"
      ],
      "metadata": {
        "id": "ifmIfBw_EzH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c29c9006-4493-40fd-85f0-0de5e41239d0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2c0e6b74fe12>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZrV80Gh4cDxT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}